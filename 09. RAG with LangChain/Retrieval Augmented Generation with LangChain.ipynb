{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a3e4a0-c5f4-4b45-9417-e5c0ee93ea7f",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab191b07-5684-45e6-ad92-7dba26bf693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Use current working directory and go one level up\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import your config\n",
    "from config import api_key\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7912dcf-069c-4cfb-8f88-fb718e3a78d8",
   "metadata": {},
   "source": [
    "## Chapter 1 - Building RAG applications with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cafb293-38da-48f3-8326-cba62d5c96d2",
   "metadata": {},
   "source": [
    "### Section 1.1 - Loading documents for RAG with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e8d8f-d9bc-49db-a441-69fd4ae6aa03",
   "metadata": {},
   "source": [
    "#### Loading PDF files for RAG\n",
    "To begin implementing Retrieval Augmented Generation (RAG), you'll first need to load the documents that the model will access. These documents can come from a variety of sources, and LangChain supports document loaders for many of them.\n",
    "\n",
    "In this exercise, you'll use a document loader to load a PDF document containing the paper, Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks by Lewis et al. (2021). This file is available for you as `'rag_paper.pdf'`.\n",
    "\n",
    "Note: `pypdf`, a dependency for loading PDF documents in LangChain, has already been installed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82174084-91b1-429a-bc0b-21994eb3ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Create a document loader for rag_paper.pdf\n",
    "loader = PyPDFLoader('./data/rag-paper.pdf')\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b807bfdd-ba79-4c23-8833-4ea3880cd1d4",
   "metadata": {},
   "source": [
    "#### Loading HTML files for RAG\n",
    "\n",
    "It's possible to load documents from many different formats, including complex formats like HTML.\n",
    "\n",
    "If you're not familiar with HTML, it's a markup language for creating web pages. Here's a small example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39affcc4-4466-4fcb-8efa-7d1602a7ec31",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "  <h2>Heading</h2>\n",
    "  <p>Here's some text and an image below:</p>\n",
    "  <img src=\"image.jpg\" alt=\"...\" width=\"104\" height=\"142\">\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ebdaa-f6c2-4d78-a22d-6c570dd6f486",
   "metadata": {},
   "source": [
    "In this exercise, you'll load an HTML file taken containing a DataCamp blog post webpage. The necessary classes have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672e181-1490-43c9-ae37-f321c3d6171f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "# Create a document loader for unstructured HTML\n",
    "loader = UnstructuredHTMLLoader('./data/datacamp-blog.html')\n",
    "\n",
    "# Print the first document's content\n",
    "print(data[0].page_content)\n",
    "\n",
    "# Print the first document's metadata\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08b50e-9fcf-4b8c-94a4-4f12784cc454",
   "metadata": {},
   "source": [
    "### Section 1.2 - Text splitting, embedding and vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6f3c0-9c58-444b-9e14-1085c6a61c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53ced4-b14f-44d7-949c-2cf4fa3ae835",
   "metadata": {},
   "source": [
    "#### Getting started with text splitting\n",
    "\n",
    "Time to start splitting! You've been provided with a statement about RAG stored in the string variable `text`. Your job is to split this string on occurrences of the `'.'` character. Take a look at the splitting results to see how this strategy performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fbe114-c865-45cc-af99-3436b61bafcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''RAG (retrieval augmented generation) is an advanced NLP model that combines retrieval mechanisms with generative capabilities. RAG aims to improve the accuracy and relevance of its outputs by grounding responses in precise, contextually appropriate data.'''\n",
    "\n",
    "# Define a text splitter that splits on the '.' character\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\".\",\n",
    "    chunk_size=75,  \n",
    "    chunk_overlap=10  \n",
    ")\n",
    "\n",
    "# Split the text using text_splitter\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(chunks)\n",
    "print([len(chunk) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f53a80-57e1-466c-8c06-23bc3e1638ed",
   "metadata": {},
   "source": [
    "#### Recursively splitting documents\n",
    "\n",
    "Splitting on a single character is simple and predictable, but it often produces sub-optimal chunks. In this exercise, you'll apply recursive character splitting to split the Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks paper you loaded in a earlier exercise.\n",
    "\n",
    "Recall that recursive character splitting iterates over a list of characters, splitting on each in turn to see if chunks can be created beneath the `chunk_size` limit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5aba7b-0787-4fc0-84ac-50f396ae2d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"./data/rag-paper.pdf\")\n",
    "document = loader.load()\n",
    "\n",
    "# Define a text splitter that splits recursively through the character list\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n', '.', ' ', ''],\n",
    "    chunk_size=1000,  \n",
    "    chunk_overlap=100  \n",
    ")\n",
    "\n",
    "# Split the document using text_splitter\n",
    "chunks = text_splitter.split_documents(document)\n",
    "print(chunks)\n",
    "print([len(chunk.page_content) for chunk in chunks])\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac325065-d399-4848-9fd0-0990db92ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.search(\"What is BART?\", search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa03fd02-9e4a-4e94-b43f-17677bf91fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.get(limit=1)['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d2922-9e86-40a2-8268-a5a75f97b4ea",
   "metadata": {},
   "source": [
    "#### Embedding and storing documents\n",
    "The final step for preparing the documents for retrieval is embedding and storing them. You'll be using the text-embedding-3-small model from OpenAI for embedding the chunked documents, and storing them in a local Chroma vector database.\n",
    "\n",
    "The `chunks` you created from splitting the Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks paper recursively have been pre-loaded.\n",
    "\n",
    "Creating and using an OpenAI API key is not required in this exercise. You can leave the `<OPENAI_API_TOKEN>` placeholder, which will send valid requests to the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3003e9-8a3e-44ce-8851-924d84246787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize the OpenAI embedding model\n",
    "embedding_model = OpenAIEmbeddings(\n",
    "    api_key=api_key, \n",
    "    model='text-embedding-3-small')\n",
    "\n",
    "# Create a Chroma vector store and embed the chunks\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=\"./chromadb/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c216e97-1fc5-4fb3-aa00-da5fc02e5b4f",
   "metadata": {},
   "source": [
    "### Section 1.3 - Building an LCEL retrieval chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d90aec-cef5-4709-a96b-72f527a8b662",
   "metadata": {},
   "source": [
    "#### Creating the retrieval prompt\n",
    "\n",
    "A key piece of any RAG implementation is the retrieval prompt. In this exercise, you'll create a chat prompt template for your retrieval chain and test that the LLM is able to respond using only the context provided.\n",
    "\n",
    "An `llm` has already been defined for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6795f40-64dc-49a6-9b5a-45d4b721f999",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46b27c-1e2e-4b3e-b285-d4f5f86a8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Use the only the context provided to answer the following question. If you don't know the answer, reply that you are unsure.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Convert the string into a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "# Create an LCEL chain to test the prompt\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# Invoke the chain on the inputs provided\n",
    "print(chain.invoke({\"context\": \"DataCamp's RAG course was created by Meri Nova and James Chapman!\", \"question\": \"Who created DataCamp's RAG course?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a030f2-3278-45aa-9b15-0d534f01336d",
   "metadata": {},
   "source": [
    "#### Building the retrieval chain\n",
    "Now for the finale of the chapter! You'll create a retrieval chain using LangChain's Expression Language (LCEL). This will combine the vector store containing your embedded document chunks from the RAG paper you loaded earlier, a prompt template, and an LLM so you can begin talking to your documents.\n",
    "\n",
    "Here's a reminder of the prompt_template you created in the previous exercise, and which is available for you to use:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc8bb9e0-6c2f-4ca8-9b68-94aa9179c30f",
   "metadata": {},
   "source": [
    "Use the only the context provided to answer the following question. If you don't know the answer, reply that you are unsure.\n",
    "Context: {context}\n",
    "Question: {question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5fc3e7-2913-4b91-aefe-91c37aa39332",
   "metadata": {},
   "source": [
    "The vector_store of embedded document chunks that you created previously has also been loaded for you, along with all of the libraries and classes required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4e3be-f012-4386-8940-cb9bb2bb517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Convert the vector store into a retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
    "\n",
    "# Create the LCEL retrieval chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "#print(chain.invoke(\"Who are the authors?\"))\n",
    "#print(chain.invoke(\"What is BART?\"))\n",
    "print(chain.invoke(\"What is the broader impact\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60783ea-3ac5-4b2c-8f94-26774858e974",
   "metadata": {},
   "source": [
    "## Chapter 2 - Improving the RAG Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13315f87-cc56-43f7-a4d4-cc3b46dcd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483eeea3-42dd-4f9f-b0e4-b3b1b66951bd",
   "metadata": {},
   "source": [
    "#### Loading code files\n",
    "Chatbots can not only access text files, but also code files like Python `(.py)` and Markdown files `(.md)`. In this exercise, you'll load a Python file containing the RAG architecture you created in Chapter 1. Let's load the file to get a reminder!\n",
    "\n",
    "All of the classes needed to complete this exercise are already loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86585dbd-4dd2-4166-856c-c138696010dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document loader for README.md and load it\n",
    "loader = UnstructuredMarkdownLoader('./data/README.md')\n",
    "\n",
    "markdown_data = loader.load()\n",
    "print(markdown_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2523c-29a6-4a36-931b-3fcebcd08fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PythonLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707245f8-81c5-4bfd-a6f0-89f3b2615adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document loader for rag.py and load it\n",
    "loader = PythonLoader('rag.py')\n",
    "\n",
    "python_data = loader.load()\n",
    "print(python_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102fa28-c804-4d6f-8fc6-71397a35ce67",
   "metadata": {},
   "source": [
    "#### Splitting Python files\n",
    "\n",
    "Although text and code files contain the same characters, code files contain structures beyond natural language. To retain this code-specific context during document splitting, you should program the splitter to first try to split on the most common code structure. Fortunately, LangChain provides functionality to do just that!\n",
    "\n",
    "All of the necessary classes have been imported for you, including `Language` from `langchain_text_splitters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3cd48-9990-4a12-af73-6d48be2c6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# Create a Python-aware recursive character splitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=300, chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Split the Python content into chunks\n",
    "chunks = python_splitter.split_documents(python_data)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8caf756-1230-4986-b29d-06c6bca4fa57",
   "metadata": {},
   "source": [
    "### Section 2.2 - Advanced splitting method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb0fe2-c87e-41c5-b24a-21471a78f2fd",
   "metadata": {},
   "source": [
    "#### Splitting by tokens\n",
    "\n",
    "Splitting documents using RecursiveCharacterTextSplitter or CharacterTextSplitter is convenient, and can give you good performance in some cases, but it does have one drawback: they split using characters as base units, rather than tokens, which are processed by the model.\n",
    "\n",
    "In this exercise, you'll split documents using a token text splitter, so you can verify the number of tokens in each chunk to ensure that they don't exceed the model's context window. A PDF document has been loaded as `document`.\n",
    "\n",
    "`tiktoken` and all necessary classes have been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add20fc-d154-4fb7-b5a1-c33fd879264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Create a document loader for rag_paper.pdf\n",
    "loader = PyPDFLoader('./data/rag-paper.pdf')\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "#print(data[0])\n",
    "\n",
    "# Get the encoding for gpt-4o-mini\n",
    "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
    "\n",
    "# Create a token text splitter\n",
    "token_splitter = TokenTextSplitter(encoding_name=encoding.name, chunk_size=100, chunk_overlap=10)\n",
    "\n",
    "# Split the PDF into chunks\n",
    "chunks = token_splitter.split_documents(document)\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\nNo. tokens: {len(encoding.encode(chunk.page_content))}\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d46eef6-6d26-45a1-8d0e-620b5865efcc",
   "metadata": {},
   "source": [
    "#### Splitting semantically\n",
    "\n",
    "All of the splitting strategies you've used up to this point have the same drawback: the split doesn't consider the context of the surrounding text, so context can easily be lost during splitting.\n",
    "\n",
    "In this exercise, you'll create and apply a semantic text splitter, which is a cutting-edge experimental method for splitting text based on semantic meaning. When the splitter detects that the meaning of the text has deviated past a certain threshold, a split will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6f486-c9c5-46e7-8458-44dd7b9deee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Create a document loader for rag_paper.pdf\n",
    "loader = PyPDFLoader('./data/rag-paper.pdf')\n",
    "# Instantiate an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(api_key=api_key, model='text-embedding-3-small')\n",
    "\n",
    "# Create the semantic text splitter with desired parameters\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embedding_model, breakpoint_threshold_type=\"gradient\", breakpoint_threshold_amount=0.8\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "chunks = semantic_splitter.split_documents(document)\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d811ea8-d9c6-4d45-8d65-0f41a3cebad7",
   "metadata": {},
   "source": [
    "### Section 2.3 - Optimizing document retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c677d-d635-454d-8263-079c1f60fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a70b0fd-43a4-4b2f-bde2-9f96d1136f16",
   "metadata": {},
   "source": [
    "#### Understanding BM25\n",
    "\n",
    "Before you start integrating a BM25 sparse retriever into your RAG architecture, it's best to test it on some short strings to get a intuition for how the retriever selects the documents.\n",
    "\n",
    "You've been provided with three strings that you'll use as the basis for your BM25 retriever. The functionality required for this exercise is already loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a89a1-9c6f-424c-b41c-44acc4f248e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [\n",
    "    \"RAG stands for Retrieval Augmented Generation.\",\n",
    "    \"Graph Retrieval Augmented Generation uses graphs to store and utilize relationships between documents in the retrieval process.\",\n",
    "    \"There are different types of RAG architectures; for example, Graph RAG.\"\n",
    "]\n",
    "\n",
    "# Initialize the BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_texts(chunks, k=3)\n",
    "\n",
    "# Invoke the retriever\n",
    "results = bm25_retriever.invoke(\"Graph RAG\")\n",
    "\n",
    "# Extract the page content from the first result\n",
    "print(\"Most Relevant Document:\")\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a5755-27b0-4c77-8782-094cbaabba1e",
   "metadata": {},
   "source": [
    "#### Sparse retrieval with BM25\n",
    "\n",
    "Time to try out a sparse retrieval implementation! You'll create a BM25 retriever to ask questions about an academic paper on RAG, which has already been split into chunks called chunks. An OpenAI chat model and prompt have also been defined as `llm` and `prompt`, respectively. You can view the prompt provided by printing it in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64667bb-a04b-4f69-bbd1-62730ff02ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f503d37d-16b3-49e0-9579-6dbb21753212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document loader for rag_paper.pdf\n",
    "loader = PyPDFLoader('./data/rag-paper.pdf')\n",
    "# Instantiate an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(api_key=api_key, model='text-embedding-3-small')\n",
    "\n",
    "# Create the semantic text splitter with desired parameters\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=embedding_model, breakpoint_threshold_type=\"gradient\", breakpoint_threshold_amount=0.8\n",
    ")\n",
    "\n",
    "# Split the document\n",
    "chunks = semantic_splitter.split_documents(document)\n",
    "# print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce19cd5-1d1f-49f9-a1b4-fbc5817c0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_string  =\"\"\"\n",
    "Use the only the context provided to answer the following question. If you don't know the answer, reply that you are unsure.\n",
    "Context: {context}\n",
    "Question: {question}\\n\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7410b89c-3aa7-4fba-a28c-68d2264e27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BM25 retriever from chunks\n",
    "retriever = BM25Retriever.from_documents(\n",
    "    documents=chunks, \n",
    "    k=5\n",
    ")\n",
    "\n",
    "# Create the LCEL retrieval chain\n",
    "chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "         | prompt\n",
    "         | llm\n",
    "         | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "print(chain.invoke(\"What are knowledge-intensive tasks?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84cc110-22fe-4cfb-964b-fda16301b2e4",
   "metadata": {},
   "source": [
    "### Section 2.4 - Introduction to RAG evaluation\n",
    "\n",
    "**remark** - I found the video unclear. The matter is rather complex is very briefly introduced and too difficult to capture from the video. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3452d5-27af-4355-a624-f305ef4db4fd",
   "metadata": {},
   "source": [
    "#### Ragas context precision evaluation\n",
    "To start your RAG evaluation journey, you'll begin by evaluating the context precision RAG metric using the `ragas` framework. Recall that context precision is essentially a measure of how relevant the retrieved documents are to the input query.\n",
    "\n",
    "In this exercise, you've been provided with an input query, and the documents retrieved by a RAG application, and the ground truth, which was the most appropriate document to retrieve based on the opinion of a human expert. You'll calculate the context precision on these strings before evaluating an actual LangChain RAG chain in the next exercise.\n",
    "\n",
    "The text generated by the RAG application has been saved to the variable `model_response` for brevity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd0f2d-5e63-4e7e-953d-5046f6bd8ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key, temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5050cd35-ff28-4bec-ac61-b68b6a0d8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import context_precision\n",
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "\n",
    "# Define the context precision chain\n",
    "context_precision_chain = EvaluatorChain(metric=context_precision, llm=llm, embeddings=embeddings)\n",
    "\n",
    "# Evaluate the context precision of the RAG chain\n",
    "eval_result = context_precision_chain({\n",
    "  \"question\": \"How does RAG enable AI applications?\",\n",
    "  \"ground_truth\": \"RAG enables AI applications by integrating external data in generative models.\",\n",
    "  \"contexts\": [\n",
    "    \"RAG enables AI applications by integrating external data in generative models.\",\n",
    "    \"RAG enables AI applications such as semantic search engines, recommendation systems, and context-aware chatbots.\"\n",
    "  ]\n",
    "})\n",
    "\n",
    "print(f\"Context Precision: {eval_result['context_precision']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c67e4be-46a1-41ae-8b5e-b96c3d5eaae7",
   "metadata": {},
   "source": [
    "#### Ragas faithfulness evaluation\n",
    "\n",
    "In this exercise, you'll evaluate the faithfulness of the RAG architecture you created at the end of Chapter 1. This chain has been re-defined for you and is available as through the variable `chain`.\n",
    "\n",
    "You'll use the query provided, the chain's output, and the retrieved output to evaluate the faithfulness using the `ragas` framework.\n",
    "\n",
    "The classes required have already been imported for you."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b2b6ca3-e7c6-43ab-9597-f0bfec923f01",
   "metadata": {},
   "source": [
    "from ragas.metrics import faithfulness\n",
    "\n",
    "# Query the retriever using the query and extract the document text\n",
    "query = \"How does RAG improve question answering with LLMs?\"\n",
    "retrieved_docs = [doc.page_content for doc in retriever.invoke(query)]\n",
    "\n",
    "# Define the faithfulness chain\n",
    "faithfulness_chain = EvaluatorChain(metric=faithfulness, llm=llm, embeddings=embeddings)\n",
    "\n",
    "# Evaluate the faithfulness of the RAG chain\n",
    "eval_result = faithfulness_chain({\n",
    "  \"question\": query,\n",
    "  \"answer\": chain.invoke(query),\n",
    "  \"contexts\": retrieved_docs\n",
    "})\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c8aeb5-2304-46b1-8041-73b822ab55e7",
   "metadata": {},
   "source": [
    "#### String evaluation\n",
    "Time to really evaluate the final output by comparing it to an answer written by a subject matter expert. You'll use LangSmith's `LangChainStringEvaluator` class to perform this string comparison evaluation.\n",
    "\n",
    "A `prompt_template` for string evaluation has already been written for you as:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9239deaf-1a20-4e44-92cd-66c87410e67c",
   "metadata": {},
   "source": [
    "You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:{query}\n",
    "Here is the real answer:{answer}\n",
    "You are grading the following predicted answer:{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d83a184-9239-438c-99fc-52f1c1e27e4d",
   "metadata": {},
   "source": [
    "The output from the RAG chain is stored as `predicted_answer` and the expert's response is stored as `ref_answer`.\n",
    "\n",
    "All of the necessary classes have been imported for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776b239-35c5-411a-a38e-f2a3fff1f0d4",
   "metadata": {},
   "source": [
    "**remark** - below we need to do quite some setup for the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519c25e-5d62-43a7-b6fe-4985e786dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "eval_llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key, temperature=0)\n",
    "\n",
    "prompt = \"\"\"You are an expert professor specialized in grading students' answers to questions.\n",
    "You are grading the following question:{query}\n",
    "Here is the real answer:{answer}\n",
    "You are grading the following predicted answer:{result}\n",
    "Respond with CORRECT or INCORRECT:\n",
    "Grade:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\", \"result\"],\n",
    "    template= prompt)\n",
    "\n",
    "prompt_template\n",
    "\n",
    "query = \"How does RAG improve question answering with LLMs\"\n",
    "predicted_answer = \"RAG improves question answering with LLMs by generating correct answers even when the correct answer is not present in any retrieved document, achieving a notable accuracy of 11.8% in such cases, while extractive models would score 0%. Additionally, RAG models outperform other models like BART in terms of generating factually correct and diverse text, as well as being able to answer questions in a more flexible, abstractive manner rather than relying solely on extractive methods.\"\n",
    "ref_answer = \"Retrieval-Augmented Generation (RAG) improves question answering with large language models (LLMs) by combining a retrieval mechanism with a generative model. The retrieval system fetches relevant documents or passages from external knowledge sources, giving the LLM access to more up-to-date and accurate information than what it has learned during training. This allows RAG to generate responses that are grounded in factual data, reducing the risk of hallucination and improving the model's accuracy, especially in niche or specialized domains where the LLM alone may lack expertise. By leveraging both external knowledge and the generative abilities of LLMs, RAG enhances the quality, relevance, and factuality of the answers provided.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db07a1-c97d-46af-96a4-a2fa70b8e8e4",
   "metadata": {},
   "source": [
    "**exercise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6309ac25-7adb-4b02-b7b5-3daadc8892e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the QA string evaluator\n",
    "qa_evaluator = LangChainStringEvaluator(\n",
    "    \"qa\",\n",
    "    config={\n",
    "        \"llm\": eval_llm,\n",
    "        \"prompt\": prompt_template\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"How does RAG improve question answering with LLMs?\"\n",
    "\n",
    "# Evaluate the RAG output by evaluating strings\n",
    "score = qa_evaluator.evaluator.evaluate_strings(\n",
    "    prediction=predicted_answer,\n",
    "    reference=ref_answer,\n",
    "    input=query\n",
    ")\n",
    "\n",
    "print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f23eb2f-334e-4bad-8165-63cc73c245e1",
   "metadata": {},
   "source": [
    "oh dear! Looks like this RAG application needs to go back to the drawing board. Perhaps some of the techniques learned in this chapter, like semantic splitting or sparse retrieval, would improve this metric, or perhaps tweaking the retrieval prompt to allow a bit more creativity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac53dde-e38c-4cee-a5fa-85bd12f3986a",
   "metadata": {},
   "source": [
    "## Chapter 3 - Introduction to Graph RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631c515-607b-41f6-8e09-dc0992e64b03",
   "metadata": {},
   "source": [
    "### Section 3.1 - From vectors to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed8f4d-cc7f-41f7-bf83-a0113df02290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3a0872-575a-4840-b73d-00ad872827ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Original document\n",
    "doc = Document(page_content=\"\"\"\n",
    "The 20th century witnessed the rise of some of the most influential scientists in history, \n",
    "with Albert Einstein and Marie Curie standing out among them. Einstein, best known for his theory of relativity, \n",
    "revolutionized our understanding of space, time, and energy, earning him the Nobel Prize in Physics in 1921 \n",
    "for his explanation of the photoelectric effect. Marie Curie, a pioneer in the study of radioactivity, \n",
    "was the first woman to win a Nobel Prize. She was awarded the Nobel Prize in Physics in 1903, \n",
    "shared with her husband Pierre Curie and Henri Becquerel, for their work on radiation. \n",
    "Curie later made history again by winning a second Nobel Prize in Chemistry in 1911 \n",
    "for her discoveries of radium and polonium. Both scientists made monumental contributions \n",
    "that continue to influence the fields of physics and beyond.\n",
    "\"\"\")\n",
    "\n",
    "# Splitter configuration\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "\n",
    "# Split into smaller docs\n",
    "docs = splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a518d-c9ec-42f6-a484-f85d97887d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7056c-7756-42d6-969e-4d0fc447396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LLM\n",
    "llm = ChatOpenAI(api_key=api_key, model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Instantiate the LLM graph transformer\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# Convert the text documents to graph documents\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(docs)\n",
    "print(f\"Derived Nodes:\\n{graph_documents[0].nodes}\\n\")\n",
    "print(f\"Derived Edges:\\n{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83324c4d-1fcb-4eca-b9fa-9dce718573f4",
   "metadata": {},
   "source": [
    "#### Getting to know graphs\n",
    "\n",
    "Graphs of nodes and edges are to Graph RAG what vectors and vector distances are to vector RAG. Graphs, however, are much better able to capture the relationship between different entities in the text, rather than relying on semantic meaning to return of the relevant context.\n",
    "\n",
    "Consider the small graph that closely resembles that one you created in the previous exercise.\n",
    "\n",
    "Which of the following statements about the graph shown is correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9482deb6-9c9e-4fbc-a3b1-e8644014c7b9",
   "metadata": {},
   "source": [
    "### Section 3.2 - Storing and quering documents\n",
    "\n",
    "**Intermezzo - setting up neo4j using docker**\n",
    "\n",
    "visit docker hub for more details: https://hub.docker.com/_/neo4j"
   ]
  },
  {
   "cell_type": "raw",
   "id": "645637c8-fdc3-4e17-ac75-7e90b90fb60e",
   "metadata": {},
   "source": [
    "docker run \\\n",
    "  --name neo4j \\\n",
    "  -p7474:7474 -p7687:7687 \\\n",
    "  -e NEO4J_AUTH=neo4j/ThisIsAStrongPassword123! \\\n",
    "  -e NEO4J_PLUGINS='[\"apoc\"]' \\\n",
    "  -e NEO4J_dbms_security_procedures_unrestricted=apoc.* \\\n",
    "  -e NEO4J_dbms_security_procedures_whitelist=apoc.* \\\n",
    "  -e NEO4J_apoc_import_file_enabled=true \\\n",
    "  -e NEO4J_apoc_export_file_enabled=true \\\n",
    "  neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290c0e8-f1ff-4217-b341-6c1921747ec0",
   "metadata": {},
   "source": [
    "This runs Neo4j in a container with:\n",
    "\n",
    "- HTTP interface at http://localhost:7474\n",
    "- Bolt protocol on port 7687 (used by Python client)\n",
    "- Default username: neo4j\n",
    "- Password: ThisIsAStrongPassword123!\n",
    "\n",
    "please note that I did not include persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357404d-2db8-46fa-92c8-52fcfa20370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Connection URI and credentials\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"ThisIsAStrongPassword123!\"\n",
    "\n",
    "# Create driver\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# Example query\n",
    "def create_and_query(tx):\n",
    "    # Create a node\n",
    "    tx.run(\"CREATE (:Person {name: $name})\", name=\"Ada Lovelace\")\n",
    "    # Return all people\n",
    "    result = tx.run(\"MATCH (p:Person) RETURN p.name AS name\")\n",
    "    return [record[\"name\"] for record in result]\n",
    "\n",
    "# Run the query\n",
    "with driver.session() as session:\n",
    "    names = session.execute_write(create_and_query)\n",
    "    print(\"People in the database:\", names)\n",
    "\n",
    "# Close the driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78cf79-6920-47af-84ab-d5dc835d22d9",
   "metadata": {},
   "source": [
    "**Neo4j connect langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6adc5f-dac4-471c-8119-a16059807c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import Neo4jGraph\n",
    "#from langchain_neo4j import Neo4jGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2d096-0a18-4b98-8a95-c4948f2b2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=\"ThisIsAStrongPassword123!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cb1a80-d2ba-4bfe-871c-ead208991fa8",
   "metadata": {},
   "source": [
    "#### Building-up your graph database\n",
    "So that you don't have to regenerate your graph documents every time, it's best practice to store them in a database that's specifically designed for graph data. Neo4j graph databases are an excellent choice for graph storage and retrieval, so you'll set one up using LangChain's Neo4j functionality.\n",
    "\n",
    "Note: to use Neo4j in LangChain, you must also have the `neo4j` library installed as a dependency. In this course, this has already been done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d47973-4f4d-4870-b2e0-79f130520865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the graph documents, sources, and include entity labels\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    include_source=True,\n",
    "    baseEntityLabel=True\n",
    ")\n",
    "\n",
    "graph.refresh_schema()\n",
    "\n",
    "# Print the graph schema\n",
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685c396-f149-4cdd-b556-e540d8b57de7",
   "metadata": {},
   "source": [
    "#### Querying your graph database\n",
    "Now that you have your database set up, it's time to begin querying. You'll view the graph schema to refamiliarize yourself with the nodes and relationships, and then write a Cypher query to query the graph.\n",
    "\n",
    "If you need help with writing or fixing your Cypher query, you can use the `ask_chatgpt(text: str)` function that we've defined for you, which accepts a string argument.\n",
    "\n",
    "The `graph` you created previously is available for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd39096-d2a1-47f3-b731-d59de0e447d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the graph schema\n",
    "print(graph.get_schema)\n",
    "\n",
    "# Query the graph\n",
    "results = graph.query(\"\"\"\n",
    "MATCH (relativity:Concept {id: \"Theory Of Relativity\"}) <-[:KNOWN_FOR]- (scientist)\n",
    "RETURN scientist\n",
    "\"\"\")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7b3d6-34ca-47a8-96f5-a86643f0cea6",
   "metadata": {},
   "source": [
    "### Section 3.3 - Creating the Graph RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad924a-ab71-4a43-b727-ffc1787760cc",
   "metadata": {},
   "source": [
    "#### Chaining, Graph RAG style!\n",
    "Now to bring everything together to create a Graph RAG QA chain! You've been provided with the same `graph` you've worked with throughout this chapter (with some potential variation in the specific nodes and relationships), and you'll connect this with another LLM to generate the Cypher query and return the natural language response."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b57cc7ad-ffa3-4170-8f41-5e68b859ccd2",
   "metadata": {},
   "source": [
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "\n",
    "# Create the Graph Cypher QA chain\n",
    "graph_qa_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=ChatOpenAI(api_key=api_key, temperature=0), graph=graph, verbose=True\n",
    ")\n",
    "\n",
    "# Invoke the chain with the input provided\n",
    "result = graph_qa_chain.invoke({\"query\": \"Who discovered the element Radium?\"})\n",
    "\n",
    "# Print the result text\n",
    "print(f\"Final answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632fa6e1-dad8-4666-8dde-de1b964b88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(graph.get_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d298487-c1b3-435d-b0ef-1586ee2dbb45",
   "metadata": {},
   "source": [
    "### Section 3.4 - improving graph retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d97bfa-3659-460d-9351-48d2bf97a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs.neo4j_graph import Neo4jGraph\n",
    "from langchain_community.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # Initialize the Neo4jGraph\n",
    "# graph = Neo4jGraph(\n",
    "#     url=\"bolt://localhost:7687\",\n",
    "#     username=\"neo4j\",\n",
    "#     password=\"ThisIsAStrongPassword123!\"\n",
    "# )\n",
    "\n",
    "# Create the Graph Cypher QA chain\n",
    "graph_qa_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=ChatOpenAI(api_key=api_key, temperature=0), \n",
    "    graph=graph, \n",
    "    allow_dangerous_requests=True,\n",
    "    validate_cypher=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Invoke the chain with the input provided\n",
    "result = graph_qa_chain.invoke({\"query\": \"Who discovered the element Radium?\"})\n",
    "\n",
    "# Print the result text\n",
    "print(f\"Final answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a4dc62-c36d-4be3-95b7-ca99efba4850",
   "metadata": {},
   "source": [
    "#### Graph RAG with filtering\n",
    "\n",
    "For large and complex graphs, LLMs can sometimes struggle to accurately infer the most relevant nodes and relationships to build the Cypher query. Quite often, you will only need the LLM to be aware of a subset of the graph, and excluding particular node types will not only make it easier for the LLM to accurately create the Cypher query, but it will improve the query latency.\n",
    "\n",
    "The graph database you've been working with is available as `graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc6561-c0f2-49b4-a884-ec6caa42e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph QA chain excluding Concept\n",
    "graph_qa_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    graph=graph, \n",
    "    allow_dangerous_requests=True,\n",
    "    verbose=True, \n",
    "    exclude_types=[\"Concept\"]\n",
    ")\n",
    "\n",
    "# Invoke the chain with the input provided\n",
    "result = graph_qa_chain.invoke({\"query\": \"Who was Marie Curie married to?\"})\n",
    "print(f\"Final answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9bb02-27b5-447e-a034-97d487956534",
   "metadata": {},
   "source": [
    "#### Validating Cypher queries\n",
    "\n",
    "When the LLMs generate the Cypher query, they have the graph schema available for reference; however, this doesn't mean there's absolute certainty that the query will reflect the schema perfectly. To improve reliability, you can validate and fix the generated query against the schema, which is particularly well-suited to fixing incorrect relationship directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176e232-af8e-4457-9e3a-918432215e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph QA chain, validating the generated Cypher query\n",
    "graph_qa_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    graph=graph, \n",
    "    verbose=True,\n",
    "    validate_cypher=True,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "\n",
    "# Invoke the chain with the input provided\n",
    "result = graph_qa_chain.invoke({\"query\": \"Who won the Nobel Prize In Physics?\"})\n",
    "print(f\"Final answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce0685c-dbe3-4bfb-97cf-238b94160868",
   "metadata": {},
   "source": [
    "#### Creating a Cypher few-shot prompt\n",
    "\n",
    "The final technique you'll utilize to improve the reliability of the generated Cypher is providing a few-shot prompt. Few-shot prompts are a great way of steering a model toward a desired output without needing to fine-tune it on a large dataset of examples.\n",
    "\n",
    "A set of `examples` tailored to this particular use case is available as examples; feel free to print it in the shell to view its contents. You'll use these to create a few-shot prompt for the Cypher generation process. The graph you created before is still available as `graph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e5a40b-c21e-4941-b798-b863961b683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [{'question': 'How many scientists are mentioned in the graph?',\n",
    "  'query': 'MATCH (p:Person) RETURN count(DISTINCT p)'}]\n",
    "\n",
    "# Create an example prompt template\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"User input: {question}\\nCypher query: {query}\"\n",
    ")\n",
    "\n",
    "# Create the few-shot prompt template\n",
    "cypher_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"You are a Neo4j expert. Given an input question, create a syntactically correct Cypher query to run.\\n\\nHere is the schema information\\n{schema}.\\n\\nBelow are a number of examples of questions and their corresponding Cypher queries.\",\n",
    "    suffix=\"User input: {question}\\nCypher query: \",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "# Create the graph Cypher QA chain\n",
    "graph_qa_chain = GraphCypherQAChain.from_llm(\n",
    "    graph=graph, \n",
    "    llm=llm, \n",
    "    cypher_prompt=cypher_prompt,\n",
    "    verbose=True, \n",
    "    validate_cypher=True,\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "# Invoke the chain with the input provided\n",
    "result = graph_qa_chain.invoke({\"query\": \"Which scientist proposed the Theory Of Relativity?\"})\n",
    "print(f\"Final answer: {result['result']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d4af0-a336-462e-9836-ac02a87ecf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
