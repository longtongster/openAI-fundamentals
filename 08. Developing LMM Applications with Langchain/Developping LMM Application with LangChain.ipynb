{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cbc9aad-1d7f-4281-9b85-1be84e9144dc",
   "metadata": {},
   "source": [
    "# Chapter 1 - Developing LMM Application with LangChain\n",
    "\n",
    "Welcome to the LangChain framework for building applications on LLMs! You'll learn about the main components of LangChain, including models, chains, agents, prompts, and parsers. You'll create chatbots using both open-source models from Hugging Face and proprietary models from OpenAI, create prompt templates, and integrate different chatbot memory strategies to manage context and resources during conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0acea5-9756-4853-8ea9-ac7a0d575455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Use current working directory and go one level up\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import your config\n",
    "from config import api_key\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19892af0-bba1-46fb-a4f4-5088642f30ae",
   "metadata": {},
   "source": [
    "### Section 1.1 - The langChain ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a537a-b94c-4db9-8e8b-23687bae21d1",
   "metadata": {},
   "source": [
    "#### OpenAI models in LangChain!\n",
    "\n",
    "OpenAI's GPT-series models are some of the highest performing LLMs around. They are available via OpenAI's API, which you can easily interact with using LangChain.\n",
    "\n",
    "Normally, using OpenAI's models would require a personal API key used for billing the cost of the models. In this course, you do not need to create or provide an OpenAI API key. The `\"<OPENAI_API_TOKEN>\"` placeholder will send valid requests to the API. If you make large number of requests in a short period, you may encounter a `RateLimitError`. If you see this, please pause for a moment and try again.\n",
    "\n",
    "The `ChatOpenAI` class has already been imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89746693-d1fe-4f4f-884f-acad76062abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "prompt = 'Three reasons for using LangChain for LLM application development.'\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e2721-817c-4fde-a050-d6778e0860f4",
   "metadata": {},
   "source": [
    "#### Hugging Face models in LangChain!\n",
    "\n",
    "There are thousands of models freely available to download and use on Hugging Face. Hugging Face integrates really nicely into LangChain via its partner library, `langchain-huggingface`, which is available for you to use.\n",
    "\n",
    "In this exercise, you'll load and call the `crumb/nano-mistral` model from Hugging Face. This is a ultra-light LLM designed to be fine-tuned for greater performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f1d16-641f-4d7d-a209-60baae001c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the class for defining Hugging Face pipelines\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# Define the LLM from the Hugging Face model ID\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"crumb/nano-mistral\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 20}\n",
    ")\n",
    "\n",
    "prompt = \"Hugging Face is\"\n",
    "\n",
    "# Invoke the model\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0629144c-2387-4b2c-ab76-6b8550d4ebf2",
   "metadata": {},
   "source": [
    "### Section 1.2 - Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f72df-f6df-411e-ab11-bb1617b59d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4838ef0b-6691-439f-8e18-ae87270aab2a",
   "metadata": {},
   "source": [
    "#### Prompt templates and chaining\n",
    "\n",
    "In this exercise, you'll begin using two of the core components in LangChain: prompt templates and chains!\n",
    "\n",
    "The classes necessary for completing this exercise, including `ChatOpenAI`, have been pre-loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f61fc1-3f7e-4e6d-bab4-b0a63fc674cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template=template\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\t\n",
    "\n",
    "# Create a chain to integrate the prompt template and LLM\n",
    "llm_chain = prompt | llm\n",
    "\n",
    "# Invoke the chain on the question\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "response = llm_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a649eb9e-59a5-4f51-8926-9116f7b4e3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbb65df-2d0c-486d-8c1a-a0da3766b3b8",
   "metadata": {},
   "source": [
    "#### Chat prompt templates\n",
    "\n",
    "Given the importance of chat models in many LLM applications, LangChain provides functionality for creating prompt templates to structure messages to different chat roles.\n",
    "\n",
    "The `ChatPromptTemplate` class has already been imported for you, and an LLM has already been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1801447-ff9d-4338-b804-5294690862f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a geography expert that returns the colors present in a country's flag.\"),\n",
    "        (\"human\", \"France\"),\n",
    "        (\"ai\", \"blue, white, red\"),\n",
    "        (\"human\", \"{country}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"country\": \"France\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2b8ad-1bd6-4eb8-9365-d0ccd4d73f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt.model_dump()['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e2b6c3-a006-4121-ba40-77df75921e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain the prompt template and model, and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "\n",
    "country = \"Japan\"\n",
    "response = llm_chain.invoke({\"country\": country})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ace97-2ec0-45e0-a520-7914dff4633c",
   "metadata": {},
   "source": [
    "### Section 1.3 - Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a5a56-2d93-4091-b486-23c9cb3a9347",
   "metadata": {},
   "source": [
    "#### Creating the few-shot example set\n",
    "PromptTemplate and ChatPromptTemplate are great for integrating variables, but struggle with integrating datasets containing many examples. This is where `FewShotPromptTemplate` comes in! In this exercise, you'll create a dataset, in the form of a list of dictionaries, to contain the follow question-answer pairs.\n",
    "\n",
    "- Question: How many DataCamp courses has Jack completed?\n",
    "- Answer: 36\n",
    "- Question: How much XP does Jack have on DataCamp?\n",
    "- Answer: 284,320XP\n",
    "- Question: What technology does Jack learn about most on DataCamp?\n",
    "- Answer: Python\n",
    "- \n",
    "In the next exercise, you'll convert this information into a few-shot prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d6687-e95a-43e9-9bdb-480f16afcff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the examples list of dicts\n",
    "examples = [\n",
    "  {\n",
    "    \"question\": \"How many DataCamp courses has Jack completed?\",\n",
    "    \"answer\": \"36\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"How much XP does Jack have on DataCamp?\",\n",
    "    \"answer\": \"284,320XP\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"What technology does Jack learn about most on DataCamp?\",\n",
    "    \"answer\": \"Python\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0a2a91-5d84-4540-9e4e-39f334ddefa9",
   "metadata": {},
   "source": [
    "#### Building the few-shot prompt template\n",
    "\n",
    "With your examples in a structured format, it's now time to create the few-shot prompt template! You'll create a template that convert the question-answer pairs into the following format:\n",
    "\n",
    "```\n",
    "Question: Example question\n",
    "Example Answer\n",
    "\n",
    "``` \n",
    "All of the LangChain classes necessary for completing this exercise have been pre-loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c008f064-884d-442b-8bd3-5e1d9b76f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "# Complete the prompt for formatting answers\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\")\n",
    "\n",
    "# Create the few-shot prompt\n",
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"input\": \"What is Jack's favorite technology on DataCamp?\"})\n",
    "print(prompt.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df926d9a-5dbb-45fa-9870-913da2b63b7c",
   "metadata": {},
   "source": [
    "#### Implementing few-shot prompting\n",
    "Time to combine your components together into a chain! The few-shot prompt you created in the previous exercise is still available for you to use, along with `examples` and `example_prompt`.\n",
    "\n",
    "All of the LangChain classes necessary for completing this exercise have been pre-loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57827026-3c32-4057-943f-d8124b7a076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "# Create an OpenAI chat LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n",
    "\n",
    "# Create and invoke the chain\n",
    "llm_chain = prompt_template | llm\n",
    "response = llm_chain.invoke({\"input\": \"What is Jack's favorite technology on DataCamp?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f675b8-58e9-4155-a9b0-66dec0286965",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc32da34-f121-40db-869b-11ab42f2d0f5",
   "metadata": {},
   "source": [
    "## Chapter 2 - Chains and Agents\n",
    "\n",
    "Time to level up your LangChain chains! You'll learn to use the LangChain Expression Language (LCEL) for defining chains with greater flexibility. You'll create sequential chains, where inputs are passed between components to create more advanced applications. You'll also begin to integrate agents, which use LLMs for decision-making.\n",
    "\n",
    "### Section 2.1 - Sequential chains\n",
    "\n",
    "#### Building prompts for sequential chains\n",
    "\n",
    "Over the next couple of exercises, you'll work to create a system for helping people learn new skills. This system needs to be built sequentially, so learners can modify plans based on their preferences and constraints. You'll utilize your LangChain LCEL skills to build a sequential chain to build this system, and the first step is to design the prompt templates that will be used by this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9491d0-e7bb-45d1-beb0-21189f042aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template that takes an input activity\n",
    "learning_prompt = PromptTemplate(\n",
    "    input_variables=[\"activity\"],\n",
    "    template=\"I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?\"\n",
    ")\n",
    "\n",
    "# Create a prompt template that places a time constraint on the output\n",
    "time_prompt = PromptTemplate(\n",
    "    input_variables=['learning_plan'],\n",
    "    template=\"I only have one week. Can you create a plan to help me hit this goal: {learning_plan}.\"\n",
    ")\n",
    "\n",
    "# Invoke the learning_prompt with an activity\n",
    "print(learning_prompt.invoke({\"activity\": \"play golf\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351f65d-72d1-4017-816c-3c6359787bd9",
   "metadata": {},
   "source": [
    "#### Sequential chains with LCEL\n",
    "With your prompt templates created, it's time to tie everything together, including the LLM, using chains and LCEL. An `llm` has already been defined for you that uses OpenAI's gpt-4o-mini model\n",
    "\n",
    "For the final step of calling the chain, feel free to insert any activity you wish! If you're struggling for ideas, try inputting `\"play the harmonica\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffc36ab-2b2b-473b-81b1-5c0d53f4152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Create an OpenAI chat LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aad8bc-6861-4be2-b0ec-5d77d33657c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_prompt = PromptTemplate(\n",
    "    input_variables=[\"activity\"],\n",
    "    template=\"I want to learn how to {activity}. Can you suggest how I can learn this step-by-step?\"\n",
    ")\n",
    "\n",
    "time_prompt = PromptTemplate(\n",
    "    input_variables=[\"learning_plan\"],\n",
    "    template=\"I only have one week. Can you create a concise plan to help me hit this goal: {learning_plan}.\"\n",
    ")\n",
    "\n",
    "# Complete the sequential chain with LCEL\n",
    "seq_chain = ({\"learning_plan\": learning_prompt | llm | StrOutputParser()}\n",
    "    | time_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "# Call the chain\n",
    "print(seq_chain.invoke({\"activity\": \"lay the harmonica\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f472801-0e94-4a8f-b77a-efbdc1bbe338",
   "metadata": {},
   "source": [
    "so basically the first part of the pipeline is get a response on learning the activity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7c8ff-1d6b-41df-8ee9-0ff724b8f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "(learning_prompt | llm | StrOutputParser()).invoke(\"play golf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a7188-789e-4031-a3d1-02c4f0d635b0",
   "metadata": {},
   "source": [
    "### Section 2.2 - Introduction to LangChain agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afce5df-826a-42d7-a9e5-974f996510dd",
   "metadata": {},
   "source": [
    "#### ReAct agents\n",
    "\n",
    "Time to have a go at creating your own ReAct agent! Recall that ReAct stands for Reason and Act, which describes how they make decisions. In this exercise, you'll load the built-in `wikipedia` tool to integrate external data from Wikipedia with your LLM. An `llm` has already been defined for you that uses OpenAI's gpt-4o-mini model\n",
    "\n",
    "Note: The `wikipedia` tool requires the `wikipedia` Python library to be installed as a dependency, which has been done for you in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098be743-8537-4ee9-bb58-ccf920e91ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa79dd3-ebc9-465d-b1cb-574cb55bca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b348ae-dd12-485b-b88e-72f4bd6cdc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tools\n",
    "tools = load_tools([\"wikipedia\"])\n",
    "\n",
    "# Define the agent\n",
    "agent = create_react_agent(llm, tools)\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent.invoke({\"messages\": [(\"human\", \"How many people live in Amsterdam?\")]})\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdb2ba-030f-4835-8416-1f6ff95e35a8",
   "metadata": {},
   "source": [
    "### Section 2.2 - Custom tools for agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1135e99f-79d7-4699-873c-8b02f90e39eb",
   "metadata": {},
   "source": [
    "#### Defining a function for tool use\n",
    "You're working for a SaaS (software as a service) company with big goals for rolling out tools to help employees at all levels of the organization to make data-informed decisions. You're creating a PoC for an application that allows customer success managers to interface with company data using natural language to retrieve important customer data.\n",
    "\n",
    "You've been provided with a pandas DataFrame called customers that contains a small sample of `customer` data. Your first step in this project is to define a Python function to extract information from this table given a customer's name. `pandas` has already been imported as `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdfd84-5fad-4785-989c-30b71e02922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4184a9bd-7dfc-4771-9b49-516d7bb86780",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = pd.read_csv('customers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71fe27-6270-4eb4-8938-0d857c140022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to retrieve customer info by-name\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    # Filter customers for the customer's name\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "  \n",
    "# Call the function on Peak Performance Co.\n",
    "print(retrieve_customer_info(\"Peak Performance Co.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d67d407-ca87-43eb-b628-688f71e0f6cc",
   "metadata": {},
   "source": [
    "#### Creating custom tools\n",
    "\n",
    "Now that you have a function for extracting customer data from the `customers` DataFrame, it's time to convert this function into a tool that's compatible with LangChain agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fdd39-f88b-42a3-84df-49b65310489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de821898-1757-4178-800d-94f9e9975305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the retrieve_customer_info function into a tool\n",
    "@tool\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "  \n",
    "# Print the tool's arguments\n",
    "print(retrieve_customer_info.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38799c9-c810-460d-9143-19731f8c033d",
   "metadata": {},
   "source": [
    "#### Integrating custom tools with agents\n",
    "\n",
    "Now that you have your tools at-hand, it's time to set up your agentic workflow! You'll again be using a ReAct agent, which, recall, reasons on the steps it should take, and selects tools using this context and the tool descriptions. An `llm` has already been defined for you that uses OpenAI's `gpt-4o-mini` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719146d-8a6f-4c57-a2f5-b704131b9170",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retrieve_customer_info(name: str) -> str:\n",
    "    \"\"\"Retrieve customer information based on their name.\"\"\"\n",
    "    customer_info = customers[customers['name'] == name]\n",
    "    return customer_info.to_string()\n",
    "\n",
    "# Create a ReAct agent\n",
    "agent = create_react_agent(llm, tools=[retrieve_customer_info])\n",
    "\n",
    "# Invoke the agent on the input\n",
    "messages = agent.invoke({\"messages\": [(\"human\", \"Create a summary of our customer: Peak Performance Co.\")]})\n",
    "print(messages['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee3b3f-eadf-419c-a37e-417d4a6b690b",
   "metadata": {},
   "source": [
    "## Chapter 3 - Retreival Augmented Generation (RAG)\n",
    "\n",
    "One limitation of LLMs is that they have a knowledge cut-off due to being trained on data up to a certain point. In this chapter, you'll learn to create applications that use Retrieval Augmented Generation (RAG) to integrate external data with LLMs. The RAG workflow contains a few different processes, including splitting data, creating and storing the embeddings using a vector database, and retrieving the most relevant information for use in the application. You'll learn to master the entire workflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a5bb6-3b8f-4033-9ff7-32c91be77d8f",
   "metadata": {},
   "source": [
    "### Section 3.1. - Integrating document loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37b29a9-d648-4580-86d6-58ae6d98171a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader, UnstructuredHTMLLoader\n",
    "loader = CSVLoader('customers.csv')\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab68c8-c12c-4467-b17e-ba12e151f36a",
   "metadata": {},
   "source": [
    "#### PDF document loaders\n",
    "\n",
    "To begin implementing Retrieval Augmented Generation (RAG), you'll first need to load the documents that the model will access. These documents can come from a variety of sources, and LangChain supports document loaders for many of them.\n",
    "\n",
    "In this exercise, you'll use a document loader to load a PDF document containing the paper, RAG VS Fine-Tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture by Balaguer et al. (2024).\n",
    "\n",
    "Note: `pypdf`, a dependency for loading PDF documents in LangChain, has already been installed for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1dabd-81d1-4451-99e6-5012cefdc3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Create a document loader for rag_vs_fine_tuning.pdf\n",
    "loader = PyPDFLoader(\"./data/rag_vs_fine_tuning.pdf\")\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975f7e1e-af39-4a1e-b45a-0d40a48650ec",
   "metadata": {},
   "source": [
    "#### CSV document loaders\n",
    "\n",
    "Comma-separated value (CSV) files are an extremely common file format, particularly in data-related fields. Fortunately, LangChain provides different document loaders for different formats, keeping almost all of the syntax the same!\n",
    "\n",
    "In this exercise, you'll use a document loader to load a CSV file containing data on FIFA World Cup international viewership. If you're interested in the full analysis behind this data, check out How to Break FIFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21652580-cc95-4dfb-85b5-6490e513b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Create a document loader for fifa_countries_audience.csv\n",
    "loader = CSVLoader(\"./data/fifa_countries_audience.csv\")\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28318689-0c8b-4ac1-bf6e-b72543ba4b6a",
   "metadata": {},
   "source": [
    "#### HTML document loaders\n",
    "\n",
    "It's possible to load documents from many different formats, including complex formats like HTML.\n",
    "\n",
    "In this exercise, you'll load an HTML file containing a White House executive order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61272294-1947-4150-a7bd-bde247521980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "# Create a document loader for unstructured HTML\n",
    "loader = UnstructuredHTMLLoader(\"./data/white_house_executive_order_nov_2023.html\")\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "\n",
    "# Print the first document\n",
    "print(data[0])\n",
    "\n",
    "# Print the first document's metadata\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2868cc-bfed-4ebd-bd7f-ec9934eefd4e",
   "metadata": {},
   "source": [
    "### Section 3.2 - Splitting external data for retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14972460-b210-4303-a837-107d49263224",
   "metadata": {},
   "source": [
    "#### Splitting by character\n",
    "A key process in implementing Retrieval Augmented Generation (RAG) is splitting documents into chunks for storage in a vector database.\n",
    "\n",
    "There are several splitting strategies available in LangChain, some with more complex routines than others. In this exercise, you'll implement a character text splitter, which splits documents based on characters and measures the chunk length by the number of characters.\n",
    "\n",
    "Remember that there is no ideal splitting strategy, you may need to experiment with a few to find the right one for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb0bc3-2065-4b0c-b204-c08f5b4ad0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the character splitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=24,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "\n",
    "# Split the string and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218141e-5ff3-4154-9b19-0915d6bf02d7",
   "metadata": {},
   "source": [
    "#### Recursively splitting by character\n",
    "\n",
    "Many developers are using a recursive character splitter to split documents based on a specific list of characters. These characters are paragraphs, newlines, spaces, and empty strings, by default: `[\"\\n\\n\", \"\\n\", \" \", \"\"]`.\n",
    "\n",
    "Effectively, the splitter tries to split by paragraphs, checks to see if the `chunk_size` and `chunk_overlap` values are met, and if not, splits by sentences, then words, and individual characters.\n",
    "\n",
    "Often, you'll need to experiment with different `chunk_size` and chunk_overlap values to find the ones that work well for your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec429114-480e-4bd3-af03-fba52f900864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the recursive character splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)\n",
    "print([len(doc) for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbd9167-122a-4a37-b6fb-6887c0536e6a",
   "metadata": {},
   "source": [
    "#### Splitting HTML\n",
    "In this exercise, you'll split an HTML containing an executive order on AI created by the US White House in October 2023. To retain as much context as possible in the chunks, you'll split using larger chunk_size and chunk_overlap values.\n",
    "\n",
    "All of the LangChain classes necessary for completing this exercise have been pre-loaded for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8860277-ab59-44bc-a498-dac2ea9dfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HTML document into memory\n",
    "loader = UnstructuredHTMLLoader(\"./data/white_house_executive_order_nov_2023.html\")\n",
    "data = loader.load()\n",
    "\n",
    "# Define variables\n",
    "chunk_size = 300\n",
    "chunk_overlap = 100\n",
    "\n",
    "# Split the HTML\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    separators=['.'])\n",
    "\n",
    "docs = splitter.split_documents(data) \n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b981c2-849f-4666-a4e9-304132408b3b",
   "metadata": {},
   "source": [
    "### Section 3.3 - RAG storage and retrieval using vector databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0a50c-fe1f-4802-a6a3-e61775e1016d",
   "metadata": {},
   "source": [
    "#### Preparing the documents and vector database\n",
    "Over the next few exercises, you'll build a full RAG workflow to have a conversation with a PDF document containing the paper, RAG VS Fine-Tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture by Balaguer et al. (2024). This works by splitting the documents into chunks, storing them in a vector database, defining a prompt to connect the retrieved documents and user input, and building a retrieval chain for the LLM to access this external data.\n",
    "\n",
    "In this exercise, you'll prepare the document for storage and ingest them into a Chroma vector database. You'll use a RecursiveCharacterTextSplitter to chunk the PDF, and ingest them into a Chroma vector database using an OpenAI embeddings function. As with the rest of the course, you don't need to provide your own OpenAI API key.\n",
    "\n",
    "The following classes have already been imported for you: `RecursiveCharacterTextSplitter`, `Chroma`, and `OpenAIEmbeddings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7c696-9c98-4323-aa79-86fa90687b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc8eaf2-f767-4b10-a424-fc890db2522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('./data/rag_vs_fine_tuning.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "# Split the document using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Embed the documents in a persistent Chroma vector database\n",
    "embedding_function = OpenAIEmbeddings(api_key=api_key, model='text-embedding-3-small')\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=embedding_function,\n",
    "    persist_directory=\"./chroma\"\n",
    ")\n",
    "\n",
    "# Configure the vector store as a retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e86c61-d7ea-4078-b3dc-8038d8e6b857",
   "metadata": {},
   "source": [
    "#### Building a retrieval prompt template\n",
    "Now your documents have been ingested into vector database and are ready for retrieval, you'll need to design a chat prompt template to combine the retrieved document chunks with the user input question.\n",
    "\n",
    "The general structure of the prompt has already been provided; your goal is to insert the correct input variable placeholders into the `message` string and convert the string into a chat prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a3e14-394e-4377-a0b7-accd83fd0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add placeholders to the message string\n",
    "message = \"\"\"\n",
    "Answer the following question using the context provided:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a chat prompt template from the message string\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"human\", message)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa466288-39dc-403a-b3a3-19e3a7a2abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1de0bb-b588-44f1-9519-9e4a62249a3c",
   "metadata": {},
   "source": [
    "#### Creating a RAG chain\n",
    "Now to bring all the components together in your RAG workflow! You've prepared the documents and ingested them into a Chroma database for retrieval. You created a prompt template to include the retrieved chunks from the academic paper and answer questions.\n",
    "\n",
    "The prompt template you created in the previous exercise is available as prompt_template, an OpenAI model has been initialized as `llm`, and the code to recreate your `retriever` has be included in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb2e62-b6cc-46f8-96c5-4477f1fa4665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding=OpenAIEmbeddings(api_key=api_key, model='text-embedding-3-small'),\n",
    "    persist_directory=\"./chroma/\"\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "# Create a chain to link retriever, prompt_template, and llm\n",
    "rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm)\n",
    "\n",
    "# Invoke the chain\n",
    "response = rag_chain.invoke(\"Which popular LLMs were considered in the paper?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc90079-b105-499f-8a71-0a01c586cb41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
